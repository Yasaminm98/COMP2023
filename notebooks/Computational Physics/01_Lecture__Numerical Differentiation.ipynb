{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "A primer on numerical differentiation\n",
                "========================\n",
                "\n",
                "In order to numerically evaluate a derivative $y'(x)=dy/dx$ at point $x_0$, we approximate is by using finite differences:\n",
                "Therefore we find: \n",
                "\n",
                "$dx \\approx \\Delta x =x_1-x_0$\n",
                "\n",
                "$dy \\approx \\Delta y =y_1-y_0= y(x_1)-y(x_0) = y(x_0+\\Delta_x)-y(x_0)$\n",
                "\n",
                "Then we re-write the derivative in terms of discrete differences as:\n",
                "$$\\frac{dy}{dx} \\approx \\frac{\\Delta y}{\\Delta x}$$\n",
                "\n",
                "#### Example\n",
                "\n",
                "Let's look at the accuracy of this approximation in terms of the interval $\\Delta x$. In our first example we will evaluate the derivative of $y=x^2$ at $x=1$."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "dx =   1.0000000000000000, d =   3.0000000000000000, err =   1.0000000000000000\n",
                        "dx =   0.1000000000000000, d =   2.1000000000000019, err =   0.1000000000000019\n",
                        "dx =   0.0100000000000000, d =   2.0100000000000007, err =   0.0100000000000007\n",
                        "dx =   0.0010000000000000, d =   2.0009999999996975, err =   0.0009999999996975\n",
                        "dx =   0.0001000000000000, d =   2.0000999999991720, err =   0.0000999999991720\n",
                        "dx =   0.0000100000000000, d =   2.0000100000139298, err =   0.0000100000139298\n",
                        "dx =   0.0000010000000000, d =   2.0000009999243669, err =   0.0000009999243669\n",
                        "dx =   0.0000001000000000, d =   2.0000001010878061, err =   0.0000001010878061\n",
                        "dx =   0.0000000100000000, d =   1.9999999878450576, err =  -0.0000000121549424\n",
                        "dx =   0.0000000010000000, d =   2.0000001654807416, err =   0.0000001654807416\n",
                        "dx =   0.0000000001000000, d =   2.0000001654807416, err =   0.0000001654807416\n"
                    ]
                }
            ],
            "source": [
                "dx, x = 1, 1\n",
                "while(dx > 1e-10):\n",
                "    dy = (x+dx)**2 - x**2\n",
                "    d = dy / dx\n",
                "    print(\"dx = %20.16f, d = %20.16f, err = %20.16f\" % (dx, d, d-2))\n",
                "    dx = dx / 10"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Why is it that the sequence does not converge? This is due to the round-off errors in the representation of the floating point numbers. To see this, we can simply type:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.0002000099999999172"
                        ]
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "((1+0.0001)*(1+0.0001) - 1*1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's try using powers of 1/2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "dx =   1.0000000000000000, d =   3.0000000000000000, err =   1.0000000000000000\n",
                        "dx =   0.5000000000000000, d =   2.5000000000000000, err =   0.5000000000000000\n",
                        "dx =   0.2500000000000000, d =   2.2500000000000000, err =   0.2500000000000000\n",
                        "dx =   0.1250000000000000, d =   2.1250000000000000, err =   0.1250000000000000\n",
                        "dx =   0.0625000000000000, d =   2.0625000000000000, err =   0.0625000000000000\n",
                        "dx =   0.0312500000000000, d =   2.0312500000000000, err =   0.0312500000000000\n",
                        "dx =   0.0156250000000000, d =   2.0156250000000000, err =   0.0156250000000000\n",
                        "dx =   0.0078125000000000, d =   2.0078125000000000, err =   0.0078125000000000\n",
                        "dx =   0.0039062500000000, d =   2.0039062500000000, err =   0.0039062500000000\n",
                        "dx =   0.0019531250000000, d =   2.0019531250000000, err =   0.0019531250000000\n",
                        "dx =   0.0009765625000000, d =   2.0009765625000000, err =   0.0009765625000000\n",
                        "dx =   0.0004882812500000, d =   2.0004882812500000, err =   0.0004882812500000\n",
                        "dx =   0.0002441406250000, d =   2.0002441406250000, err =   0.0002441406250000\n",
                        "dx =   0.0001220703125000, d =   2.0001220703125000, err =   0.0001220703125000\n",
                        "dx =   0.0000610351562500, d =   2.0000610351562500, err =   0.0000610351562500\n",
                        "dx =   0.0000305175781250, d =   2.0000305175781250, err =   0.0000305175781250\n",
                        "dx =   0.0000152587890625, d =   2.0000152587890625, err =   0.0000152587890625\n",
                        "dx =   0.0000076293945312, d =   2.0000076293945312, err =   0.0000076293945312\n",
                        "dx =   0.0000038146972656, d =   2.0000038146972656, err =   0.0000038146972656\n",
                        "dx =   0.0000019073486328, d =   2.0000019073486328, err =   0.0000019073486328\n",
                        "dx =   0.0000009536743164, d =   2.0000009536743164, err =   0.0000009536743164\n",
                        "dx =   0.0000004768371582, d =   2.0000004768371582, err =   0.0000004768371582\n",
                        "dx =   0.0000002384185791, d =   2.0000002384185791, err =   0.0000002384185791\n",
                        "dx =   0.0000001192092896, d =   2.0000001192092896, err =   0.0000001192092896\n",
                        "dx =   0.0000000596046448, d =   2.0000000596046448, err =   0.0000000596046448\n",
                        "dx =   0.0000000298023224, d =   2.0000000298023224, err =   0.0000000298023224\n",
                        "dx =   0.0000000149011612, d =   2.0000000149011612, err =   0.0000000149011612\n",
                        "dx =   0.0000000074505806, d =   2.0000000000000000, err =   0.0000000000000000\n",
                        "dx =   0.0000000037252903, d =   2.0000000000000000, err =   0.0000000000000000\n",
                        "dx =   0.0000000018626451, d =   2.0000000000000000, err =   0.0000000000000000\n",
                        "dx =   0.0000000009313226, d =   2.0000000000000000, err =   0.0000000000000000\n",
                        "dx =   0.0000000004656613, d =   2.0000000000000000, err =   0.0000000000000000\n",
                        "dx =   0.0000000002328306, d =   2.0000000000000000, err =   0.0000000000000000\n",
                        "dx =   0.0000000001164153, d =   2.0000000000000000, err =   0.0000000000000000\n"
                    ]
                }
            ],
            "source": [
                "dx, x = 1, 1\n",
                "while(dx > 1e-10):\n",
                "    dy = (x+dx)**2 - x**2\n",
                "    d = dy / dx\n",
                "    print(\"dx = %20.16f, d = %20.16f, err = %20.16f\" % (dx, d, d-2))\n",
                "    dx = dx / 2"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In addition, one could consider the midpoint difference, defined as:\n",
                "$$ dy \\approx \\Delta y = y(x_0+\\frac{\\Delta_x}{2})-y(x_0-\\frac{\\Delta_x}{2}).$$\n",
                "\n",
                "For a more complex function we need to import it from the math module. For instance, let's calculate the derivative of $sin(x)$ at $x=\\pi/4$, including both the forward and midpoint differences."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1.00000e+00   0.2699544827129282  -0.4371522984736194   0.6780100988420897  -0.0290966823444578\n",
                        "5.00000e-01   0.5048856975964859  -0.2022210835900616   0.6997640691250939  -0.0073427120614536\n",
                        "2.50000e-01   0.6118351194488110  -0.0952716617377366   0.7052667953545546  -0.0018399858319930\n",
                        "1.25000e-01   0.6611301360648314  -0.0459766451217162   0.7066465151141266  -0.0004602660724210\n",
                        "6.25000e-02   0.6845566203276618  -0.0225501608588857   0.7069916978116630  -0.0001150833748845\n",
                        "3.12500e-02   0.6959440534591259  -0.0111627277274217   0.7070780092891873  -0.0000287718973603\n",
                        "1.56250e-02   0.7015538499518499  -0.0055529312346977   0.7070995881463489  -0.0000071930401987\n",
                        "7.81250e-03   0.7043374663312676  -0.0027693148552800   0.7071049829223881  -0.0000017982641595\n",
                        "3.90625e-03   0.7057239167465070  -0.0013828644400405   0.7071063316202526  -0.0000004495662950\n",
                        "1.95312e-03   0.7064157978737740  -0.0006909833127736   0.7071066687949497  -0.0000001123915979\n",
                        "9.76562e-04   0.7067614018394579  -0.0003453793470897   0.7071067530886239  -0.0000000280979237\n",
                        "4.88281e-04   0.7069341196006462  -0.0001726615859013   0.7071067741621846  -0.0000000070243630\n",
                        "2.44141e-04   0.7070204574170020  -0.0000863237695455   0.7071067794304327  -0.0000000017561149\n",
                        "1.22070e-04   0.7070636210582961  -0.0000431601282515   0.7071067807473810  -0.0000000004391666\n",
                        "6.10352e-05   0.7070852015622222  -0.0000215796243254   0.7071067810775276  -0.0000000001090200\n",
                        "3.05176e-05   0.7070959914854029  -0.0000107897011447   0.7071067811593821  -0.0000000000271655\n",
                        "1.52588e-05   0.7071013863678672  -0.0000053948186803   0.7071067811775720  -0.0000000000089756\n",
                        "7.62939e-06   0.7071040837763576  -0.0000026974101900   0.7071067811775720  -0.0000000000089756\n",
                        "3.81470e-06   0.7071054324915167  -0.0000013486950309   0.7071067811921239   0.0000000000055763\n",
                        "1.90735e-06   0.7071061068563722  -0.0000006743301754   0.7071067811921239   0.0000000000055763\n",
                        "9.53674e-07   0.7071064440533519  -0.0000003371331957   0.7071067811921239   0.0000000000055763\n",
                        "4.76837e-07   0.7071066126227379  -0.0000001685638097   0.7071067811921239   0.0000000000055763\n",
                        "2.38419e-07   0.7071066969074309  -0.0000000842791167   0.7071067811921239   0.0000000000055763\n",
                        "1.19209e-07   0.7071067392826080  -0.0000000419039395   0.7071067811921239   0.0000000000055763\n",
                        "5.96046e-08   0.7071067616343498  -0.0000000195521977   0.7071067821234465   0.0000000009368989\n",
                        "2.98023e-08   0.7071067728102207  -0.0000000083763269   0.7071067802608013  -0.0000000009257463\n",
                        "1.49012e-08   0.7071067765355110  -0.0000000046510366   0.7071067765355110  -0.0000000046510366\n",
                        "7.45058e-09   0.7071067839860916   0.0000000027995440   0.7071067839860916   0.0000000027995440\n",
                        "3.72529e-09   0.7071067988872528   0.0000000177007052   0.7071067690849304  -0.0000000121016172\n",
                        "1.86265e-09   0.7071067690849304  -0.0000000121016172   0.7071067690849304  -0.0000000121016172\n",
                        "9.31323e-10   0.7071068286895752   0.0000000475030276   0.7071067094802856  -0.0000000717062619\n",
                        "4.65661e-10   0.7071068286895752   0.0000000475030276   0.7071068286895752   0.0000000475030276\n",
                        "2.32831e-10   0.7071070671081543   0.0000002859216067   0.7071070671081543   0.0000002859216067\n",
                        "1.16415e-10   0.7071075439453125   0.0000007627587649   0.7071065902709961  -0.0000001909155515\n"
                    ]
                }
            ],
            "source": [
                "from math import sin, sqrt, pi\n",
                "dx = 1.\n",
                "while(dx > 1.e-10):\n",
                "    x = pi/4.\n",
                "    d1 = sin(x+dx) - sin(x) # \"forward\" difference\n",
                "    d2 = sin(x+dx/2.) - sin(x-dx/2.) # \"central\" difference\n",
                "    d1, d2 = d1 / dx, d2 / dx \n",
                "    print(\"%8.5e %20.16f %20.16f %20.16f %20.16f\" % (dx, d1, d1-sqrt(2)/2, d2, d2-sqrt(2)/2))\n",
                "    dx = dx / 2"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "What do you notice? Which one does better?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "A more in-depth discussion about round-off errors in numerical differentiation can be found <a href=\"http://www.uio.no/studier/emner/matnat/math/MAT-INF1100/h10/kompendiet/kap11.pdf\">here</a>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Special functions in **numpy**\n",
                "\n",
                "numpy provides a simple method **diff()** to calculate the numerical derivatives of a dataset stored in an array by forward differences. The function **gradient()** will calculate the derivatives by midpoint (or central) difference, that provides a more accurate result. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
                        ]
                    },
                    "execution_count": 30,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "x1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[ 1.  2.  4.  6.  8. 10. 12. 14. 16. 17.]\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "[<matplotlib.lines.Line2D at 0x13a56cbd0>]"
                        ]
                    },
                    "execution_count": 32,
                    "metadata": {},
                    "output_type": "execute_result"
                },
                {
                    "data": {
                        "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA8oklEQVR4nO3dZ3hUdf7+8fekTUJIBhJIIJBGkV6TiICrsrIqKoINQUpU7Cgirgq66rorov5cxYKIgBKa4qpg13VRQKSlEHovIbSEmkkhk2Tm/B+4m/+iiAQmc2Yy9+u65kFOzuTcFxNm7sz5zPlaDMMwEBEREfGQALMDiIiIiH9R+RARERGPUvkQERERj1L5EBEREY9S+RARERGPUvkQERERj1L5EBEREY9S+RARERGPCjI7wC+5XC4OHDhAREQEFovF7DgiIiJyFgzDoLi4mLi4OAICzvzehteVjwMHDhAfH292DBERETkH+fn5NG/e/Iz7eF35iIiIAH4OHxkZaXIaERERORt2u534+Pjq1/Ez8bry8d9TLZGRkSofIiIiPuZsRiY0cCoiIiIepfIhIiIiHqXyISIiIh6l8iEiIiIepfIhIiIiHqXyISIiIh6l8iEiIiIepfIhIiIiHqXyISIiIh6l8iEiIiIepfIhIiIiHqXyISIiIh7ldQvLiYiISO04UuJgfmY+J8oqePKa9qblUPkQERGpwwzDIGfvCWav2MNX6w9R4XQRHGjh7kta0jjCakomlQ8REZE6qLzSyWe5B5i1cg8b9turt3eJb0B6z0Qiw8yrACofIiIidcjeo2XMWZXH/Mx8ik5WAhASFMB1XeIY0TORzs0bmBsQlQ8RERGf53IZLNl+mFnL97B422EM4+ftzRuGMeyiRAalxhMVHmJuyP+h8iEiIuKjTpRV8M+sfcxZlUfe0bLq7Zdc0JgRFyXSp20MgQEWExOensqHiIiIj9mwv4jZK/L4dO1+yitdAESEBjEoNZ5hFyWS3Cjc5IRnpvIhIiLiAxxVTr5ef4hZK/aQs/dE9fa2TSJI75XEgK5x1AvxjZd130gpIiLipw6cOMm8VXv5IHMvR0oqAAgKsNCvU1NG9EwkNbEhFov3nVo5E5UPERERL2MYBit2HmXWijy+21yA0/XzBGlspJVbL0xkyIXxxESGmpzy3Kl8iIiIeIni8koWrNnPrBV57Cgsqd7eIzmKET2TuKJDLMGBvr8yisqHiIiIybYXFDNrRR6f5OyjtMIJQL2QQG7o3ozhFyXRpkmEyQndS+VDRETEBFVOF99tKmDWijxW7Dpavb1F43BGXJTIDSnNiQwNNjFh7VH5EBER8aDDxQ4+WL2Xeav3crCoHIAAC/RtF8uInkn0bhXtcwOkNaXyISIiUst+XtztOBnL8/h6w0EqnT8PkEaHh3BLWjxDL0qkWYMwk1N6jsqHiIhILTlZ4eTT3J8HSDcd/P+Lu3WNb0B6r0Su7tQUa1CgiQnNofIhIiLiZnuOlDJnZR4fZuVjL68CwFq9uFsSnZrbTE5oLpUPERERN3C6DJZsK2TWijwWbz1cvT0+KoxhPX5e3K2hFy3uZqYaf1h46dKl9O/fn7i4OCwWCwsXLvzVPps3b+a6667DZrMRHh5OWloae/fudUdeERERr3K8tIKpS3Zy2cs/cMfMrOricekFjXn3tlQW/7kP91zaUsXjf9T4nY/S0lK6dOnCHXfcwQ033PCr7+/cuZOLL76YkSNH8uyzzxIZGcnGjRsJDfXdK7GJiIj80vp9RcxasYfP1h7AUfXz4m6R/7O4W5KXL+5mJothGMY539liYcGCBQwcOLB62+DBgwkODmb27Nnn9DPtdjs2m42ioiIiIyPPNZqIiIjbOaqcfLX+ILNW5LHmfxZ3a980khE9ExnQtRlhIf43QAo1e/1268yHy+Xiyy+/5LHHHuPKK69kzZo1JCcnM378+FMKyv9yOBw4HI5TwouIiHiTiioXk3/YwZyVeRwt/Xlxt+BAC/06/ry4W4oPLu5mJrdeIL6wsJCSkhJeeOEFrrrqKv71r39x/fXXc8MNN7BkyZLT3mfixInYbLbqW3x8vDsjiYiInJdjpRUMm7GK1xZt52hpBU0iQxn7pwv4adwfeX1IN1KTolQ8asitp10OHDhAs2bNGDJkCPPmzave77rrriM8PJz333//Vz/jdO98xMfH67SLiIiYbkdhMXfMzGLvsTLqW4N4bmBHrunctE4s7uZupp12adSoEUFBQbRv3/6U7e3atWPZsmWnvY/VasVqtbozhoiIyHlbuu0wo+blUFxeRXxUGDPS07ggtm4t8GYWt5aPkJAQ0tLS2Lp16ynbt23bRmJiojsPJSIiUmsylu/hb19swukySEtqyNvDUoiurz+U3aXG5aOkpIQdO3ZUf717925yc3OJiooiISGBRx99lFtuuYVLLrmEPn368M033/D555+zePFid+YWERFxu0qni799vonZK/MAuLF7c56/oaNfXgK9NtV45mPx4sX06dPnV9vT09OZOXMmAO+++y4TJ05k3759tGnThmeffZYBAwac1c/XR21FRMQMRWWVjJqXw7IdR7BY4PGr2nLPJS00THqWavL6fV4Dp7VB5UNERDxt95FSRmZksutwKWHBgUwa3JUrOzQxO5ZPMW3gVERExNes2HmUe+dkU3Sykqa2UKanp9Ihzr8XfqttKh8iIuK3Pli9l78s3ECVy6BLfAOmDU8hJlLLgdQ2lQ8REfE7TpfB819tZsay3QD07xLH/93UmdBgDZZ6gsqHiIj4leLySka/v4Yf/rP67MN9L2D05a00WOpBKh8iIuI38o+VcWdGFlsLirEGBfCPQV24tnOc2bH8jsqHiIj4haw9x7hndjZHSytoHGFl2ohUusY3MDuWX1L5EBGROu+TnH2M+3g9FU4XHeIimZ6eSlNbmNmx/JbKh4iI1Fkul8HL/9rKW4t3AnBlh1hevaUr9UL08mcm/euLiEidVFZRxcPzc/l2YwEAo/q05JE/tSEgQIOlZlP5EBGROudg0UnuzMhi4wE7IYEBvHBjJ27o3tzsWPIfKh8iIlKnrM0/wZ2zsjhc7CA6PISpw1NITYoyO5b8D5UPERGpM75Yd4BHPlyLo8pFm9gIpqenEh9Vz+xY8gsqHyIi4vMMw+C1RduZ9O/tAPyxbQyvDe5KRGiwycnkdFQ+RETEp5VXOnn0o3V8vvYAACMvTuaJq9sRqMFSr6XyISIiPqvQXs5ds7NZm3+CoAALfx/YkSEXJpgdS36HyoeIiPikjQeKuDMji4NF5djCgpkyrDu9WjYyO5acBZUPERHxOf/aeIgx83Mpq3DSonE4M9LTSG4UbnYsOUsqHyIi4jMMw+DtJbt46dstGAZc3KoRk2/tjq2eBkt9icqHiIj4BEeVkyc+2cDHOfsAGH5RIk/3b09wYIDJyaSmVD5ERMTrHS1xcO+cbDL3HCfAAs/070B6rySzY8k5UvkQERGvtq2gmDtmZrLv+EkirEFMHtqdSy5obHYsOQ8qHyIi4rV+2FrIg/PWUOKoIiGqHu/elkqrmAizY8l5UvkQERGvYxgG7/20h+e+3ITLgAuTo3h7WApR4SFmRxM3UPkQERGvUul08fSnG3l/9V4ABqU257mBnQgJ0mBpXaHyISIiXuNEWQX3z81h+c6jWCzwRL923PmHZCwWXSq9LlH5EBERr7DrcAkjM7LYfaSU8JBAXhvcjb7tY82OJbVA5UNEREz3044j3DcnG3t5Fc0ahDE9PZV2TSPNjiW1ROVDRERMNXdVHk9/uhGny6B7QgOmDk+lcYTV7FhSi1Q+RETEFFVOF899uZmZy/cAMLBrHC/c2JnQ4EBzg0mtU/kQERGPs5dX8uC8NSzZdhiAP19xAaP6tNJgqZ9Q+RAREY/ae7SMkRmZbC8sITQ4gFcHdaVfp6ZmxxIPqvGHppcuXUr//v2Ji4vDYrGwcOHC39z33nvvxWKxMGnSpPOIKCIidcXq3ccYMHkZ2wtLiI208s97eql4+KEal4/S0lK6dOnC5MmTz7jfggULWLlyJXFxceccTkRE6o5/ZuUzdPpKjpdV0qmZjU9HXUyn5jazY4kJanzapV+/fvTr1++M++zfv58HH3yQb7/9lmuuueacw4mIiO9zugxe+mYLU5fuAuDqTk34x81dCQvRYKm/cvvMh8vlYvjw4Tz66KN06NDhd/d3OBw4HI7qr+12u7sjiYiISUodVYyZn8t3mwoAePCPrXi47wUEBGiw1J+5/UL5L774IkFBQYwePfqs9p84cSI2m636Fh8f7+5IIiJigv0nTnLT2yv4blMBIUEBvDa4K49c0UbFQ9xbPrKzs3nttdeYOXPmWX9cavz48RQVFVXf8vPz3RlJRERMkLP3OAPe/InNB+00qh/C+3ddxICuzcyOJV7CreXjxx9/pLCwkISEBIKCgggKCiIvL49HHnmEpKSk097HarUSGRl5yk1ERHzXp7n7GfzOSo6UOGjbJIKFo3qTktjQ7FjiRdw68zF8+HD69u17yrYrr7yS4cOHc/vtt7vzUCIi4mVcLoNJi7bz+qLtAPRtF8Okwd2ob9UlpeRUNf6NKCkpYceOHdVf7969m9zcXKKiokhISCA6OvqU/YODg2nSpAlt2rQ5/7QiIuKVTlY4+fM/1/Ll+oMA3HNJCx67qi2Bmu+Q06hx+cjKyqJPnz7VX48dOxaA9PR0Zs6c6bZgIiLiGwrs5dw1K4t1+4oIDrQwYWAnBqXpwwPy22pcPi677DIMwzjr/ffs2VPTQ4iIiI/YsL+IkRmZFNgdNKwXzNvDUujRIvr37yh+TSfiRETknHyz4SBj5udSXumiVUx9ZqSnkhgdbnYs8QEqHyIiUiOGYfDW4p3837dbAbjkgsa8eWs3IkODTU4mvkLlQ0REzlp5pZPxn6xnwZr9ANzWK4m/XNOOoEC3X7NS6jCVDxEROSuHix3cMzuLnL0nCAyw8NfrOjD8okSzY4kPUvkQEZHfteWQnZEzs9h/4iSRoUG8NTSFi1s3MjuW+CiVDxEROaNFmwsY/f4aSiucJEXXY8ZtabRsXN/sWOLDVD5EROS0DMNgxrLdTPhqM4YBPVtEM2VYdxrUCzE7mvg4lQ8REfmViioXTy3cwPysnxf7HHJhPH8b0JFgDZaKG6h8iIjIKY6XVnDvnGxW7T5GgAWevKY9d/ROOuvVykV+j8qHiIhU21FYwsiMTPKOllHfGsQbQ7rRp22M2bGkjlH5EBERAJZuO8yoeTkUl1fRvGEYM9LTaNMkwuxYUgepfIiICLNW7OHZzzfhdBmkJjbk7eEpNKpvNTuW1FEqHyIifqzK6eLZzzcxe2UeADd0b8bEGzphDQo0OZnUZSofIiJ+quhkJQ/My+HH7UcAeOyqNtx3aUsNlkqtU/kQEfFDe46UMjIjk52HSwkLDuTVW7pyVccmZscSP6HyISLiZ1bsPMp9c7M5UVZJU1so00ak0rGZzexY4kdUPkRE/Mj8zL08uWADVS6DLvENmDY8hZjIULNjiZ9R+RAR8QNOl8ELX29m2o+7Abi2c1NevrkLocEaLBXPU/kQEanjShxVPPT+GhZtKQTgoctbM6Zvaw2WimlUPkRE6rD8Y2XcmZHF1oJirEEB/N/NXbiuS5zZscTPqXyIiNRR2XnHuHtWNkdLK2gcYWXaiFS6xjcwO5aIyoeISF20YM0+Hv9oPRVOF+2bRjI9PZW4BmFmxxIBVD5EROoUl8vgH99tZfIPOwG4on0sr97SlXCrnu7Fe+i3UUSkjiirqGLs/LV8s/EQAPdd1pJHr2hDQIAGS8W7qHyIiNQBB4tOctesLDbstxMSGMDEGzpxY0pzs2OJnJbKh4iIj1ubf4K7ZmVRWOwgKjyEqcNTSEuKMjuWyG9S+RAR8WFfrjvI2A9zcVS5uCC2PjPS04iPqmd2LJEzUvkQEfFBhmHwxvc7eOW7bQD0adOY14d0IyI02ORkIr9P5UNExMeUVzp57KN1fLb2AAB39E7myWvaEajBUvERKh8iIj6ksLicu2dlk5t/gqAAC38b0JFbeySYHUukRlQ+RER8xKYDdu7MyORAUTm2sGCmDOtOr5aNzI4lUmMBNb3D0qVL6d+/P3FxcVgsFhYuXFj9vcrKSh5//HE6depEeHg4cXFxjBgxggMHDrgzs4iI3/luUwE3vb2cA0XltGgUzsJRvVU8xGfVuHyUlpbSpUsXJk+e/KvvlZWVkZOTw1NPPUVOTg6ffPIJW7du5brrrnNLWBERf2MYBm8v2cnds7Moq3DSu1U0C+7vTXKjcLOjiZwzi2EYxjnf2WJhwYIFDBw48Df3yczM5MILLyQvL4+EhN8/L2m327HZbBQVFREZGXmu0UREfJ6jysmTCzbwUfY+AIZdlMAz/TsQHFjjvxtFal1NXr9rfeajqKgIi8VCgwYNTvt9h8OBw+Go/tput9d2JBERr3estIJ7Z2ezes8xAizw9LXtSe+VhMWiT7SI76vV+lxeXs7jjz/OkCFDfrMFTZw4EZvNVn2Lj4+vzUgiIl5ve0ExAyYvY/WeY0RYg3j3tjRu652s4iF1Rq2Vj8rKSgYNGoRhGEyZMuU39xs/fjxFRUXVt/z8/NqKJCLi9RZvLeSGt5aTf+wkCVH1+OT+XlzWJsbsWCJuVSunXf5bPPLy8vj+++/PeO7HarVitVprI4aIiM8wDIOZy/fw9y824TLgwqQo3h6eQlR4iNnRRNzO7eXjv8Vj+/bt/PDDD0RHR7v7ECIidUql08VfP9vI3FV7Abg5pTkTru9ESJAGS6VuqnH5KCkpYceOHdVf7969m9zcXKKiomjatCk33XQTOTk5fPHFFzidTg4dOgRAVFQUISFq8CIi/6uorJL752Xz046jWCww7qq23H1JC813SJ1W44/aLl68mD59+vxqe3p6On/9619JTk4+7f1++OEHLrvsst/9+fqorYj4i91HShk5M5NdR0qpFxLIa4O78af2sWbHEjkntfpR28suu4wz9ZXzuGyIiIjfWL7jCPfNzaHoZCVxtlCmp6fRPk5/cIl/0NouIiIeNm/VXp7+dANVLoNuCQ2YOjyFmIhQs2OJeIzKh4iIh1Q5XUz4ajPv/bQHgAFd43jxxs6EBgeaG0zEw1Q+REQ8wF5eyej317B462EAHvnTBTzwx1YaLBW/pPIhIlLL8o+VccfMTLYXlhAaHMArg7pydaemZscSMY3Kh4hILcrcc4x7ZmdzrLSC2Egr00ak0rl5A7NjiZhK5UNEpJZ8lL2P8Z+so9Jp0LFZJNNHpNHEpsFSEZUPERE3c7kMXvp2K28v2QlAv45N+MegLtQL0VOuCKh8iIi4Vamjiofn5/KvTQUAPNCnFWP/dAEBARosFfkvlQ8RETc5cOIkIzOy2HzQTkhQAC/d2JmB3ZqZHUvE66h8iIi4QW7+Ce6alcXhYgeN6ocwdXgqKYkNzY4l4pVUPkREztNnaw/w6D/X4qhy0bZJBNPTU2nesJ7ZsUS8lsqHiMg5MgyDV/+9ndcXbQfg8rYxvDakG/WtemoVORP9DxEROQfllU4e+edavlx3EIC7L2nB41e1JVCDpSK/S+VDRKSGCu3l3DUri7X7iggKsDDh+o7ckpZgdiwRn6HyISJSAxv2F3FnRhaH7OU0qBfM28NSuKhFtNmxRHyKyoeIyFn6ZsMhHp6fy8lKJy0bh/PubWkkRoebHUvE56h8iIj8DsMweGvxTv7v260A/KF1I968tTu2sGCTk4n4JpUPEZEzcFQ5Gf/Jej7J2Q9Aes9Enrq2PUGBASYnE/FdKh8iIr/hSImDe2dnk5V3nMAAC3/t357hPZPMjiXi81Q+REROY+uhYkZmZLLv+EkiQoN4a2h3/tC6sdmxROoElQ8RkV/4YUshD76/hhJHFYnR9ZiRnkarmPpmxxKpM1Q+RET+wzAMZizbzfNfbcZlwEUtopgyNIWG4SFmRxOpU1Q+RESAiioXz3y2gfdX5wMwOC2evw3oSEiQBktF3E3lQ0T83omyCu6dk83KXcewWODJq9sx8uJkLBZdKl2kNqh8iIhf23m4hJEzM9lztIzwkEDeuLUbf2wba3YskTpN5UNE/Nay7Ue4b242xeVVNGsQxru3pdGmSYTZsUTqPJUPEfFLs1fm8dfPNuJ0GaQkNmTq8BQa1beaHUvEL6h8iIhfqXK6eO7LzcxcvgeAG7o14/kbOhEaHGhuMBE/ovIhIn7DXl7JA/PWsHTbYQAevbIN91/WUoOlIh6m8iEifiHvaCkjM7LYUVhCWHAgr97Shas6NjU7lohfUvkQkTpv1a6j3Dsnm+NllTSJDGV6eiodm9nMjiXit2p89ZylS5fSv39/4uLisFgsLFy48JTvG4bB008/TdOmTQkLC6Nv375s377dXXlFRGrkw6x8hs1YxfGySjo3t/HpA71VPERMVuPyUVpaSpcuXZg8efJpv//SSy/x+uuv8/bbb7Nq1SrCw8O58sorKS8vP++wIiJny+kyeP6rzTz20ToqnQbXdG7K/Lt7EhsZanY0Eb9X49Mu/fr1o1+/fqf9nmEYTJo0ib/85S8MGDAAgFmzZhEbG8vChQsZPHjw+aUVETkLJY4qxnywhn9vLgTgoctbM6Zvaw2WingJty5asHv3bg4dOkTfvn2rt9lsNnr06MGKFStOex+Hw4Hdbj/lJiJyrvafOMlNU5bz782FhAQF8PqQbjz8pwtUPES8iFvLx6FDhwCIjT310sSxsbHV3/uliRMnYrPZqm/x8fHujCQifiQ77zgD3lzGlkPFNKpvZf7dF3FdlzizY4nIL5i+XOP48eMpKiqqvuXn55sdSUR80MI1+xkybSVHSipo1zSSTx/oTbeEhmbHEpHTcOtHbZs0aQJAQUEBTZv+/8/PFxQU0LVr19Pex2q1YrXqksYicm5cLoNX/72NN77fAcCf2scy6ZauhFt1JQERb+XWdz6Sk5Np0qQJixYtqt5mt9tZtWoVPXv2dOehREQ4WeHkgfdzqovHvZe2ZOqwFBUPES9X4/+hJSUl7Nixo/rr3bt3k5ubS1RUFAkJCYwZM4bnnnuO1q1bk5yczFNPPUVcXBwDBw50Z24R8XOHisq5a1YW6/cXERxo4fnrO3FzqmbGRHxBjctHVlYWffr0qf567NixAKSnpzNz5kwee+wxSktLufvuuzlx4gQXX3wx33zzDaGh+my9iLjH+n1F3DkrkwK7g6jwEKYOTyEtKcrsWCJyliyGYRhmh/hfdrsdm81GUVERkZGRZscRES/z1fqDjP0wl/JKF61j6jMjPY2E6HpmxxLxezV5/daJURHxCYZhMPmHHbz8r20AXHpBY964tRuRocEmJxORmlL5EBGvV17pZNzH61iYewCAO3on88TVbQkKNP1qASJyDlQ+RMSrHS52cPfsLNbsPUFQgIVnB3RgaI9Es2OJyHlQ+RARr7X5oJ07M7LYf+IktrBgpgztTq9WjcyOJSLnSeVDRLzSvzcVMPqDNZRVOGnRKJzp6am0aFzf7Fgi4gYqHyLiVQzDYNqPu5j49RYMA3q1jGbK0BRs9TRYKlJXqHyIiNeoqHLxl4Xr+TBrHwC39kjg2es6EKzBUpE6ReVDRLzCsdIK7p2TzerdxwiwwFPXtue2XklYLBazo4mIm6l8iIjpdhQWc8fMLPYeKyPCGsQbt3bjsjYxZscSkVqi8iEiplqy7TAPzM2h2FFFfFQY76an0To2wuxYIlKLVD5ExDQZy/fw7OcbcRmQltSQt4elEF3fanYsEallKh8i4nGVThd/+3wTs1fmAXBTSnMmXN8Ra1CgyclExBNUPkTEo4rKKhk1L4dlO45gscC4q9py9yUtNFgq4kdUPkTEY3YfKWVkRia7DpdSLySQSbd05YoOTcyOJSIepvIhIh6xfOcR7puTQ9HJSuJsoUxLT6VDnM3sWCJiApUPEal176/ey1MLN1DlMuga34B3RqQQExFqdiwRMYnKh4jUGqfL4PmvNjNj2W4ArusSx0s3dSY0WIOlIv5M5UNEakVxeSWj31/DD1sPAzD2Txfw4B9babBURFQ+RMT98o+VcWdGFlsLirEGBfCPQV24tnOc2bFExEuofIiIW2XtOcY9s7M5WlpBTISVaSNS6RLfwOxYIuJFVD5ExG0+ydnHuI/XU+F00SEukunpqTS1hZkdS0S8jMqHiJw3l8vg5X9t5a3FOwG4qkMTXrmlC/VC9BQjIr+mZwYROS9lFVU8PD+XbzcWADCqT0se+VMbAgI0WCoip6fyISLn7GDRSUbOzGLTQTshgQG8eFMnru/W3OxYIuLlVD5E5Jzk5p/grllZHC52EB0ewjsjUkhJjDI7loj4AJUPEamxz9ce4M//XIujykWb2Ahm3JZK84b1zI4lIj5C5UNEzpphGLy2aDuT/r0dgD+2jeH1Id2ob9VTiYicPT1jiMhZKa908uhH6/h87QEA7vpDMuP6tSNQg6UiUkMqHyLyuwrt5dw1O5u1+ScICrAw4fqO3JKWYHYsEfFRKh8ickYbDxRxZ0YWB4vKaVAvmClDU+jZMtrsWCLiw1Q+ROQ3fbvxEGM+yOVkpZOWjcOZkZ5GUqNws2OJiI8LcPcPdDqdPPXUUyQnJxMWFkbLli35+9//jmEY7j6UiNQSwzCYsngn987J5mSlkz+0bsQn9/dW8RARt3D7Ox8vvvgiU6ZMISMjgw4dOpCVlcXtt9+OzWZj9OjR7j6ciLiZo8rJE59s4OOcfQCM6JnI09e2JyjQ7X+riIifcnv5WL58OQMGDOCaa64BICkpiffff5/Vq1e7+1Ai4mZHSxzcMzubrLzjBAZYeKZ/e0b0TDI7lojUMW7/U6ZXr14sWrSIbdu2AbB27VqWLVtGv379Tru/w+HAbrefchMRz9tWUMzAt34iK+84EaFBvHdbmoqHiNQKt7/zMW7cOOx2O23btiUwMBCn08mECRMYOnToafefOHEizz77rLtjiEgN/LC1kAfnraHEUUVidD1mpKfSKibC7FgiUke5/Z2PDz/8kLlz5zJv3jxycnLIyMjg5ZdfJiMj47T7jx8/nqKioupbfn6+uyOJyG8wDIMZy3YzcmYmJY4qeiRHsfD+3ioeIlKrLIabP4YSHx/PuHHjGDVqVPW25557jjlz5rBly5bfvb/dbsdms1FUVERkZKQ7o4nI/6h0unj60428v3ovALekxvP3gR0JCdJgqYjUXE1ev91+2qWsrIyAgFOfvAIDA3G5XO4+lIicoxNlFdw/N4flO49iscCTV7dj5MXJWCy6VLqI1D63l4/+/fszYcIEEhIS6NChA2vWrOGVV17hjjvucPehROQc7DpcwsiMLHYfKSU8JJDXh3Tj8naxZscSET/i9tMuxcXFPPXUUyxYsIDCwkLi4uIYMmQITz/9NCEhIb97f512Eak9P+04wn1zsrGXV9GsQRgzbkulbRP9PxOR81eT12+3l4/zpfIhUjvmrMzjmc824nQZdE9owDsjUmlU32p2LBGpI0yd+RAR71LldPHcl5uZuXwPAAO7xvHCjZ0JDQ40N5iI+C2VD5E6zF5eyYPz1rBk22EAHr2yDfdf1lKDpSJiKpUPkTpq79Ey7sjIZEdhCaHBAbw6qCv9OjU1O5aIiMqHSF20atdR7p2TzfGySppEhjI9PZWOzWxmxxIRAVQ+ROqcD7PyeXLBeiqdBp2b25g2IpXYyFCzY4mIVFP5EKkjnC6Dl77ZwtSluwC4plNTXr65C2EhGiwVEe+i8iFSB5Q6qnjog1z+vbkAgNGXt2bM5a0JCNBgqYh4H5UPER+3/8RJRs7MZMuhYkKCAvi/mzozoGszs2OJiPwmlQ8RH5az9zh3z8rmSImDRvWtTBuRQreEhmbHEhE5I5UPER/1ae5+Hv1oHRVVLto1jWR6eirNGoSZHUtE5HepfIj4GJfLYNK/t/H69zsA6NsultcGdyXcqv/OIuIb9Gwl4kNOVjj58z/X8uX6gwDcc2kLHruyLYEaLBURH6LyIeIjCuzl3DUri3X7iggOtPD89Z24OTXe7FgiIjWm8iHiAzbsL2JkRiYFdgcN6wUzdXgqFyZHmR1LROScqHyIeLmv1x/k4Q9zKa900TqmPjPS00iIrmd2LBGRc6byIeKlDMNg8g87ePlf2wC49ILGvHFrNyJDg01OJiJyflQ+RLxQeaWT8Z+sZ8Ga/QDc3juJJ69uR1BggMnJRETOn8qHiJc5XOzgntlZ5Ow9QWCAhb8N6MDQHolmxxIRcRuVDxEvsuWQnZEzs9h/4iSRoUFMGZZC71aNzI4lIuJWKh8iXmLR5gJGv7+G0gonyY3CmZ6eSsvG9c2OJSLidiofIiYzDIPpP+7m+a83YxjQq2U0bw3tToN6IWZHExGpFSofIiaqqHLx1MINzM/KB2DIhQn8bUAHgjVYKiJ1mMqHiEmOl1Zw75xsVu0+RoAF/nJNe27vnYTFokuli0jdpvIhYoIdhSWMzMgk72gZ9a1BvHFrN/q0iTE7loiIR6h8iHjY0m2HGTUvh+LyKuKjwpiRnsYFsRFmxxIR8RiVDxEPyli+h799sQmnyyAtqSFvD0shur7V7FgiIh6l8iHiAVVOF89+vonZK/MAuLF7c56/oSPWoECTk4mIeJ7Kh0gtKzpZyQPzcvhx+xEsFnj8qrbcc0kLDZaKiN9S+RCpRXuOlHJHRia7DpcSFhzIpMFdubJDE7NjiYiYSuVDpJas2HmU++Zmc6Kskqa2UKanp9IhzmZ2LBER06l8iNSC+Zl7eXLBBqpcBl3iGzBtRAoxEaFmxxIR8Qq1chnF/fv3M2zYMKKjowkLC6NTp05kZWXVxqFEvIrTZfDcF5t4/OP1VLkM+neJY/7dF6l4iIj8D7e/83H8+HF69+5Nnz59+Prrr2ncuDHbt2+nYcOG7j6UiFcpLq/koQ9y+X5LIQAP972A0Ze30mCpiMgvuL18vPjii8THx/Pee+9Vb0tOTnb3YUS8Sv6xMu7MyGJrQTHWoAD+MagL13aOMzuWiIhXcvtpl88++4zU1FRuvvlmYmJi6NatG9OmTfvN/R0OB3a7/ZSbiC/JzjvGwMk/sbWgmMYRVubf01PFQ0TkDNxePnbt2sWUKVNo3bo13377Lffddx+jR48mIyPjtPtPnDgRm81WfYuPj3d3JJFa80nOPoa8s4qjpRV0iIvkswd60zW+gdmxRES8msUwDMOdPzAkJITU1FSWL19evW306NFkZmayYsWKX+3vcDhwOBzVX9vtduLj4ykqKiIyMtKd0UTcxuUyePlfW3lr8U4AruwQy6u3dKVeiD5AJiL+yW63Y7PZzur12+3PlE2bNqV9+/anbGvXrh0ff/zxafe3Wq1YrVrbQnxHWUUVY+ev5ZuNhwAY1aclj/ypDQEBGiwVETkbbi8fvXv3ZuvWrads27ZtG4mJie4+lIjHHSw6yV2zstiw305IYAAv3NiJG7o3NzuWiIhPcXv5ePjhh+nVqxfPP/88gwYNYvXq1bzzzju888477j6UiEetzT/BXbOyKCx2EB0ewtThKaQmRZkdS0TE57h95gPgiy++YPz48Wzfvp3k5GTGjh3LXXfddVb3rck5IxFP+WLdAR75cC2OKhdtYiOYnp5KfFQ9s2OJiHiNmrx+10r5OB8qH+JNDMPg9UU7ePXf2wD4Y9sYXhvclYjQYJOTiYh4F1MHTkXqivJKJ499tI7P1h4AYOTFyTxxdTsCNVgqInJeVD5ETqOwuJy7Z2WTm3+CoAALfx/YkSEXJpgdS0SkTlD5EPmFTQfs3JmRyYGicmxhwUwZ1p1eLRuZHUtEpM5Q+RD5H//aeIgx83Mpq3DSolE4M25LI7lRuNmxRETqFJUPEX4eLJ26dBcvfrMFw4CLWzVi8q3dsdXTYKmIiLupfIjfc1Q5eXLBBj7K3gfA8IsSebp/e4ID3b70kYiIoPIhfu5oiYP75uSwes8xAizwTP8OpPdKMjuWiEidpvIhfmtbQTEjMzLJP3aSCGsQbw7tzqUXNDY7lohInafyIX5p8dZCHpy3hmJHFQlR9Xj3tlRaxUSYHUtExC+ofIhfMQyDmcv38PcvNuEy4MLkKN4elkJUeIjZ0URE/IbKh/iNSqeLZz7byLxVewEYlNqc5wZ2IiRIg6UiIp6k8iF+4URZBffPzWH5zqNYLDC+X1vu+kMLLBZdKl1ExNNUPqTO23W4hJEZWew+Ukp4SCCvDe5G3/axZscSEfFbKh9Spy3fcYT75uZQdLKSZg3CmJ6eSrumWi1ZRMRMKh9SZ81dlcczn26kymXQLaEB7wxPpXGE1exYIiJ+T+VD6pwqp4sJX23mvZ/2ADCwaxwv3NiZ0OBAc4OJiAig8iF1jL28ktHvr2Hx1sMA/PmKCxjVp5UGS0VEvIjKh9QZe4+WMTIjk+2FJYQGB/DKoK5c3amp2bFEROQXVD6kTli9+xj3zsnmWGkFsZFWpo1IpXPzBmbHEhGR01D5EJ/3z6x8nliwnkqnQadmNqaNSKWJLdTsWCIi8htUPsRnuVwGL367halLdgHQr2MTXhnUlbAQDZaKiHgzlQ/xSaWOKsbMz+W7TQUAPPjHVjzc9wICAjRYKiLi7VQ+xOfsP3GSOzOy2HzQTkhQAC/d2JmB3ZqZHUtERM6Syof4lDV7j3PXrGyOlDhoVD+EqcNTSUlsaHYsERGpAZUP8Rmf5u7n0Y/WUVHlom2TCKanp9K8YT2zY4mISA2pfIjXc7kMJi3azuuLtgNwedsYXhvSjfpW/fqKiPgiPXuLVztZ4eTPH63ly3UHAbjnkhY8dlVbAjVYKiLis1Q+xGsV2Mu5e1YWa/cVERxoYcLATgxKizc7loiInCeVD/FKG/YXcWdGFofs5TSoF8zbw1K4qEW02bFERMQNVD7E63yz4SAPz1/LyUonLRuH8+5taSRGh5sdS0RE3ETlQ7yGYRi8tXgn//ftVgD+0LoRk4d2JzI02ORkIiLiTgG1fYAXXngBi8XCmDFjavtQ4sMcVU4e+XBtdfFI75nIe7elqXiIiNRBtfrOR2ZmJlOnTqVz5861eRjxcUdKHNwzO5vsvOMEBlj4a//2DO+ZZHYsERGpJbX2zkdJSQlDhw5l2rRpNGyoK1DK6W05ZGfAmz+RnXeciNAgZt6epuIhIlLH1Vr5GDVqFNdccw19+/Y9434OhwO73X7KTfzD91sKuPGt5ew/cZLE6HosuL83f2jd2OxYIiJSy2rltMsHH3xATk4OmZmZv7vvxIkTefbZZ2sjhngpwzCYsWw3E77ajGHARS2imDI0hYbhIWZHExERD3D7Ox/5+fk89NBDzJ07l9DQ0N/df/z48RQVFVXf8vPz3R1JvEhFlYvxn6znuS9/Lh6D0+KZdUcPFQ8RET9iMQzDcOcPXLhwIddffz2BgYHV25xOJxaLhYCAABwOxynf+yW73Y7NZqOoqIjIyEh3RhOTHS+t4L652azcdYwACzxxdTtGXpyMxaJLpYuI+LqavH67/bTL5Zdfzvr160/Zdvvtt9O2bVsef/zxMxYPqbt2FJZwZ0Yme46WUd8axOtDuvLHtrFmxxIRERO4vXxERETQsWPHU7aFh4cTHR39q+3iH37cfpj75+ZQXF5FswZhvHtbGm2aRJgdS0RETKIrnEqtmr1iD3/9fBNOl0FKYkOmDk+hUX2r2bFERMREHikfixcv9sRhxItUOV387YtNzFqRB8AN3Zox8cZOWIN02k1ExN/pnQ9xu6KTlTwwL4cftx8B4NEr23D/ZS01WCoiIoDKh7jZniOljMzIZOfhUsKCA3n1li5c1bGp2bFERMSLqHyI26zcdZR752RzoqySJpGhTE9PpWMzm9mxRETEy6h8iFvMz9zLXxZuoNJp0KW5jWkjUomJ/P2LzImIiP9R+ZDz4nQZvPD1Zqb9uBuAazo35R83dyE0WIOlIiJyeiofcs5KHFU89P4aFm0pBOChy1szpm9rDZaKiMgZqXzIOdl3vIw7M7LYcqiYkKAAXr65C9d1iTM7loiI+ACVD6mx7Lxj3DM7myMlFTSqb2XaiBS6JTQ0O5aIiPgIlQ+pkYVr9vPYR+uocLpo1zSSGempxDUIMzuWiIj4EJUPOSsul8Er323jzR92AHBF+1hevaUr4Vb9ComISM3olUN+V1lFFY98uJavNxwC4N5LW/LYlW0ICNBgqYiI1JzKh5zRoaJy7pyVyYb9doIDLUy8oTM3pTQ3O5aIiPgwlQ/5Tev2neDOjCwKix1EhYcwdXgKaUlRZscSEREfp/Ihp/XV+oOM/TCX8koXrWPqMyM9jYToembHEhGROkDlQ05hGAZvfr+Df3y3DYDL2jTmjSHdiAgNNjmZiIjUFSofUq280snjH6/j09wDANzRO5knrm5LUGCAyclERKQuUfkQAAqLy7l7Vja5+ScICrDw7IAODO2RaHYsERGpg1Q+hE0H7NyZkcmBonJsYcFMGdqdXq0amR1LRETqKJUPP/fdpgIe+mANZRVOWjQKZ3p6Ki0a1zc7loiI1GEqH37KMAzeWbqLF77ZgmFA71bRvHVrCrZ6GiwVEZHapfLhhyqqXDy5YD3/zN4HwNAeCfz1ug4Ea7BUREQ8QOXDzxwrreDe2dms3nOMAAs8dW17buuVhMWiS6WLiIhnqHz4ke0FxYzMyGLvsTIirEG8cWs3LmsTY3YsERHxMyoffmLx1kIenLeGYkcV8VFhvJueRuvYCLNjiYiIH1L5qOMMwyBj+R7+9sUmXAZcmBTF28NTiAoPMTuaiIj4KZWPOqzS6eKvn21k7qq9ANyU0pwJ13fEGhRocjIREfFnKh91VFFZJffPy+anHUexWGDcVW25+5IWGiwVERHTqXzUQbuPlDJyZia7jpRSLySQSbd05YoOTcyOJSIiAqh81DnLdxzhvrk5FJ2sJM4WyvT0NNrHRZodS0REpJrKRx0yb9Venv50A1Uug67xDXhnRAoxEaFmxxIRETmFykcd4HQZTPhyM+/+tBuA67rE8dJNnQkN1mCpiIh4H7dfT3vixImkpaURERFBTEwMAwcOZOvWre4+jPxHcXklIzMyq4vH2D9dwGuDu6p4iIiI13J7+ViyZAmjRo1i5cqVfPfdd1RWVnLFFVdQWlrq7kP5vfxjZdw4ZTmLtx4mNDiAybd2Z/TlrfWJFhER8WoWwzCM2jzA4cOHiYmJYcmSJVxyySW/u7/dbsdms1FUVERkpAYlf0vmnmPcMzubY6UVxERYmZ6eSufmDcyOJSIifqomr9+1PvNRVFQEQFRU1Gm/73A4cDgc1V/b7fbajuTTyiqq+GB1Pi98vYUKp4sOcZFMT0+lqS3M7GgiIiJnpVbLh8vlYsyYMfTu3ZuOHTuedp+JEyfy7LPP1maMOmH3kVJmr8jjn9n5FJdXAXBVhya8cksX6oVoblhERHxHrZ52ue+++/j6669ZtmwZzZs3P+0+p3vnIz4+Xqdd+PlTLD9sKWTWyjyWbjtcvT0xuh539E5m+EWJBARovkNERMznFaddHnjgAb744guWLl36m8UDwGq1YrVaayuGTzpWWsH8zHzmrspj3/GTAFgs0KdNDCN6JnJJ68YqHSIi4rPcXj4Mw+DBBx9kwYIFLF68mOTkZHcfos5at+8EGcvz+HzdASqqXAA0qBfMLanxDO2RSEJ0PZMTioiInD+3l49Ro0Yxb948Pv30UyIiIjh06BAANpuNsDANRf5SeaWTL9cdZNbKPNbmn6je3rFZJCN6JnFdlzhds0NEROoUt898/NY1Jt577z1uu+22372/v3zUdt/xMuau2sv8zHyOlVYAEBIYwDWdmzK8ZyLd4hvoeh0iIuIzTJ35qOXLhvg0l8tg2Y4jzFqRx/dbCnD9558qzhbK0IsSuSUtnkb1Nf8iIiJ1mz6j6QH28ko+ytrHnJV57Dry/6/0enGrRgzvmcjlbWMICnT7xWZFRES8kspHLdpyyM6sFXksXLOfsgonAPWtQdyU0pxhFyXSKqa+yQlFREQ8T+XDzSqdLr7deIhZy/NYvedY9fYLYuszvGcS13drRn2r/tlFRMR/6VXQTQrs5cxbtZf3V++lsPjni6YFBli4qkMThvdMpEdylAZIRUREUPk4L4ZhsHr3MWatzOPbDYeo+s8EaeMIK0MuTODWCxNoYgs1OaWIiIh3Ufk4B6WOKhbm7mf2ijy2HCqu3p6W1JDhPZO4qkMTQoI0QCoiInI6Kh81sPNwCbNX5PFx9j6KHT8v7hYWHMjAbnEMvyiJ9nF197okIiIi7qLy8TucLoNFmwuYvTKPH7cfqd6eFF2P4T2TuCmlObawYBMTioiI+BaVj99wtMTB/Kx85q7cy/4T/39xt8vbxjC8ZxJ/aNVIi7uJiIicA5WPX8jNP8Gs5Xv4Yt1BKpw/L+7WsF4wg9LiGdYjkfgoLe4mIiJyPlQ++Hlxt8/XHmD2yjzW7Suq3t65uY0RPZO4tnNTLe4mIiLiJn5dPvKPlTFnVR4fZuZzvKwS+Hlxt2u7NGVEzyS6xjcwN6CIiEgd5Hflw+Uy+HHHEWYt38P3Wwv57zp4zRqEMfSiBG5JjSdai7uJiIjUGr8pH8XllXyYtY/ZK/aw52hZ9fY/tG7E8IsSubxdLIEaIBUREal1flM+7OVVTPhyEy4DIqxB3JT68+JuLRtrcTcRERFP8pvy0axBGHf+oQWJ0fUY2LUZ4VrcTURExBR+9Qr8xNXtzI4gIiLi97QAiYiIiHiUyoeIiIh4lMqHiIiIeJTKh4iIiHiUyoeIiIh4lMqHiIiIeJTKh4iIiHiUyoeIiIh4lMqHiIiIeJTKh4iIiHiUyoeIiIh4lMqHiIiIeJTKh4iIiHiU161qaxgGAHa73eQkIiIicrb++7r939fxM/G68lFcXAxAfHy8yUlERESkpoqLi7HZbGfcx2KcTUXxIJfLxYEDB4iIiMBisbj1Z9vtduLj48nPzycyMtKtP1tqTo+Hd9Hj4X30mHgXPR5nZhgGxcXFxMXFERBw5qkOr3vnIyAggObNm9fqMSIjI/WL40X0eHgXPR7eR4+Jd9Hj8dt+7x2P/9LAqYiIiHiUyoeIiIh4lF+VD6vVyjPPPIPVajU7iqDHw9vo8fA+eky8ix4P9/G6gVMRERGp2/zqnQ8RERExn8qHiIiIeJTKh4iIiHiUyoeIiIh4lF+Vj8mTJ5OUlERoaCg9evRg9erVZkfySxMnTiQtLY2IiAhiYmIYOHAgW7duNTuW/McLL7yAxWJhzJgxZkfxW/v372fYsGFER0cTFhZGp06dyMrKMjuWX3I6nTz11FMkJycTFhZGy5Yt+fvf/35W65fIb/Ob8jF//nzGjh3LM888Q05ODl26dOHKK6+ksLDQ7Gh+Z8mSJYwaNYqVK1fy3XffUVlZyRVXXEFpaanZ0fxeZmYmU6dOpXPnzmZH8VvHjx+nd+/eBAcH8/XXX7Np0yb+8Y9/0LBhQ7Oj+aUXX3yRKVOm8Oabb7J582ZefPFFXnrpJd544w2zo/k0v/mobY8ePUhLS+PNN98Efl5DJj4+ngcffJBx48aZnM6/HT58mJiYGJYsWcIll1xidhy/VVJSQvfu3Xnrrbd47rnn6Nq1K5MmTTI7lt8ZN24cP/30Ez/++KPZUQS49tpriY2NZcaMGdXbbrzxRsLCwpgzZ46JyXybX7zzUVFRQXZ2Nn379q3eFhAQQN++fVmxYoWJyQSgqKgIgKioKJOT+LdRo0ZxzTXXnPL/RDzvs88+IzU1lZtvvpmYmBi6devGtGnTzI7lt3r16sWiRYvYtm0bAGvXrmXZsmX069fP5GS+zesWlqsNR44cwel0Ehsbe8r22NhYtmzZYlIqgZ/fgRozZgy9e/emY8eOZsfxWx988AE5OTlkZmaaHcXv7dq1iylTpjB27FieeOIJMjMzGT16NCEhIaSnp5sdz++MGzcOu91O27ZtCQwMxOl0MmHCBIYOHWp2NJ/mF+VDvNeoUaPYsGEDy5YtMzuK38rPz+ehhx7iu+++IzQ01Ow4fs/lcpGamsrzzz8PQLdu3diwYQNvv/22yocJPvzwQ+bOncu8efPo0KEDubm5jBkzhri4OD0e58EvykejRo0IDAykoKDglO0FBQU0adLEpFTywAMP8MUXX7B06VKaN29udhy/lZ2dTWFhId27d6/e5nQ6Wbp0KW+++SYOh4PAwEATE/qXpk2b0r59+1O2tWvXjo8//tikRP7t0UcfZdy4cQwePBiATp06kZeXx8SJE1U+zoNfzHyEhISQkpLCokWLqre5XC4WLVpEz549TUzmnwzD4IEHHmDBggV8//33JCcnmx3Jr11++eWsX7+e3Nzc6ltqaipDhw4lNzdXxcPDevfu/auPnm/bto3ExESTEvm3srIyAgJOfakMDAzE5XKZlKhu8It3PgDGjh1Leno6qampXHjhhUyaNInS0lJuv/12s6P5nVGjRjFv3jw+/fRTIiIiOHToEAA2m42wsDCT0/mfiIiIX83bhIeHEx0drTkcEzz88MP06tWL559/nkGDBrF69Wreeecd3nnnHbOj+aX+/fszYcIEEhIS6NChA2vWrOGVV17hjjvuMDuabzP8yBtvvGEkJCQYISEhxoUXXmisXLnS7Eh+CTjt7b333jM7mvzHpZdeajz00ENmx/Bbn3/+udGxY0fDarUabdu2Nd555x2zI/ktu91uPPTQQ0ZCQoIRGhpqtGjRwnjyyScNh8NhdjSf5jfX+RARERHv4BczHyIiIuI9VD5ERETEo1Q+RERExKNUPkRERMSjVD5ERETEo1Q+RERExKNUPkRERMSjVD5ERETEo1Q+RERExKNUPkRERMSjVD5ERETEo1Q+RERExKP+HziGZvhTw2UnAAAAAElFTkSuQmCC",
                        "text/plain": [
                            "<Figure size 640x480 with 1 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "import numpy as np \n",
                "from matplotlib import pyplot as plt\n",
                "\n",
                "y = lambda x: x**2 \n",
                "\n",
                "x1 = np.arange(0, 10, 1)\n",
                "\n",
                "y1 = np.gradient(y(x1))\n",
                "print(y1)\n",
                "\n",
                "plt.plot(x1, np.gradient(y(x1)))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "array([ 0.,  2.,  4.,  6.,  8., 10.])"
                        ]
                    },
                    "execution_count": 25,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "x1"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Notice above that **gradient()** uses forward and backward differences at the two ends."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "More discussion about numerical differentiation, including higher order methods with error extrapolation can be found <a href=\"http://young.physics.ucsc.edu/115/diff.pdf\">here</a>. \n",
                "\n",
                "The module **scipy** also includes methods to accurately calculate derivatives:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "dx =   1.0000000000000000, d =   2.0000000000000000, err =   0.0000000000000000\n",
                        "dx =   0.1000000000000000, d =   2.0000000000000004, err =   0.0000000000000004\n",
                        "dx =   0.0100000000000000, d =   2.0000000000000018, err =   0.0000000000000018\n",
                        "dx =   0.0010000000000000, d =   1.9999999999998352, err =  -0.0000000000001648\n",
                        "dx =   0.0001000000000000, d =   1.9999999999992246, err =  -0.0000000000007754\n",
                        "dx =   0.0000100000000000, d =   2.0000000000020002, err =   0.0000000000020002\n",
                        "dx =   0.0000010000000000, d =   2.0000000000019997, err =   0.0000000000019997\n",
                        "dx =   0.0000001000000000, d =   2.0000000000575109, err =   0.0000000000575109\n",
                        "dx =   0.0000000100000000, d =   1.9999999933961727, err =  -0.0000000066038273\n",
                        "dx =   0.0000000010000000, d =   2.0000000544584391, err =   0.0000000544584391\n",
                        "dx =   0.0000000001000000, d =   2.0000001654807416, err =   0.0000001654807416\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/var/folders/wz/s4rx21_137g79thgtc1s1k3m0000gn/T/ipykernel_2109/12657976.py:8: DeprecationWarning: scipy.misc.derivative is deprecated in SciPy v1.10.0; and will be completely removed in SciPy v1.12.0. You may consider using findiff: https://github.com/maroba/findiff or numdifftools: https://github.com/pbrod/numdifftools\n",
                        "  d = derivative(y, x, dx)\n"
                    ]
                }
            ],
            "source": [
                "from scipy.misc import derivative\n",
                "\n",
                "y = lambda x: x**2\n",
                "\n",
                "dx, x = 1, 1\n",
                "\n",
                "while(dx > 1.e-10):\n",
                "    d = derivative(y, x, dx)\n",
                "    print(\"dx = %20.16f, d = %20.16f, err = %20.16f\" % (dx, d, d-2))\n",
                "    dx = dx / 10"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "One way to improve the roundoff errors is by simply using the **decimal** package"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "dx =   1.0000000000000000, d =   3.0000000000000000, err =   1.0000000000000000\n",
                        "dx =   0.1000000000000000, d =   2.1000000000000001, err =   0.1000000000000000\n",
                        "dx =   0.0100000000000000, d =   2.0099999999999998, err =   0.0100000000000000\n",
                        "dx =   0.0010000000000000, d =   2.0009999999999999, err =   0.0010000000000000\n",
                        "dx =   0.0001000000000000, d =   2.0001000000000002, err =   0.0001000000000000\n",
                        "dx =   0.0000100000000000, d =   2.0000100000000001, err =   0.0000100000000000\n",
                        "dx =   0.0000010000000000, d =   2.0000010000000001, err =   0.0000010000000000\n",
                        "dx =   0.0000001000000000, d =   2.0000000999999998, err =   0.0000001000000000\n",
                        "dx =   0.0000000100000000, d =   2.0000000099999999, err =   0.0000000100000000\n",
                        "dx =   0.0000000010000000, d =   2.0000000010000001, err =   0.0000000010000000\n",
                        "dx =   0.0000000001000000, d =   2.0000000001000000, err =   0.0000000001000000\n"
                    ]
                }
            ],
            "source": [
                "from decimal import Decimal\n",
                "\n",
                "dx = Decimal('1.')\n",
                "while( dx >= Decimal(\"1.e-10\")):\n",
                "    x = Decimal(\"1.\")\n",
                "    dy = (x+dx)**2 - x**2\n",
                "    d = dy / dx \n",
                "    print(\"dx = %20.16f, d = %20.16f, err = %20.16f\" % (dx, d, d-2))\n",
                "    dx = dx / Decimal(\"10.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Automatic Differentiation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Even better than numerical differentiation is automatic differentiation or *autodiff*, which is crucial to breakthroughs in machine learning.\n",
                "\n",
                "This is a technique that allows to evaluate the derivative of a function to machine precision, without the need to use finite differences, using the fact that autodiff package knows the analytical form of the derivative for certain functions. It then builds a computational graph that allows for the evaluation of the derivative of a function using the chain rule."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {},
            "outputs": [],
            "source": [
                "import jax \n",
                "import jax.numpy as jnp\n",
                "from jax import grad"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [],
            "source": [
                "f = lambda x: x**2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {},
            "outputs": [],
            "source": [
                "grad_f = jax.grad(f)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "The gradient of f at x = 13.00 is 26.0000000000000000\n"
                    ]
                }
            ],
            "source": [
                "x = 13.0\n",
                "print(\"The gradient of f at x = %.2f is %.16f\" % (x, grad_f(x)))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now compare this to the finite difference technique"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "dx =   1.0000000000000000, d =  27.0000000000000000, err =   1.0000000000000000\n",
                        "dx =   0.1000000000000000, d =  26.0999999999998522, err =   0.0999999999998522\n",
                        "dx =   0.0100000000000000, d =  26.0099999999994225, err =   0.0099999999994225\n",
                        "dx =   0.0010000000000000, d =  26.0009999999795127, err =   0.0009999999795127\n",
                        "dx =   0.0001000000000000, d =  26.0000999998055704, err =   0.0000999998055704\n",
                        "dx =   0.0000100000000000, d =  26.0000099984836162, err =   0.0000099984836162\n",
                        "dx =   0.0000010000000000, d =  26.0000009859595558, err =   0.0000009859595558\n",
                        "dx =   0.0000001000000000, d =  25.9999998775128844, err =  -0.0000001224871156\n",
                        "dx =   0.0000000100000000, d =  26.0000035723351139, err =   0.0000035723351139\n",
                        "dx =   0.0000000010000000, d =  26.0000092566769965, err =   0.0000092566769965\n",
                        "dx =   0.0000000001000000, d =  25.9998955698392749, err =  -0.0001044301607251\n"
                    ]
                }
            ],
            "source": [
                "dx, x = 1, 13\n",
                "while(dx > 1.e-10):\n",
                "    dy = (x+dx)**2 - x**2\n",
                "    d = dy / dx\n",
                "    print(\"dx = %20.16f, d = %20.16f, err = %20.16f\" % (dx, d, d-2*x))\n",
                "    dx = dx / 10"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "There is no error on the autodiff result because it knows $dy/dx$ as a *function*, rather than computing it by numerical approximation\n",
                "\n",
                "Ok, so that's great and all, but autodiff really gets its legs when you have a more complicated function. Derivatives of a more complicated function that is composed of many smaller functions require many applications of the chain rule. Autodiff does this for you."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "metadata": {},
            "outputs": [],
            "source": [
                "def f_complicated(x):\n",
                "    return jnp.cos(jnp.sin(jnp.tanh(x)))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "metadata": {},
            "outputs": [],
            "source": [
                "grad_fcomp = grad(f_complicated)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "Array(-0.19349389, dtype=float32, weak_type=True)"
                        ]
                    },
                    "execution_count": 52,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "grad_fcomp(1.0)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Some of the most complicated functions out there are called *neural networks* which can involve millions or billions of smaller functions or *neurons* in the composition. Autodiff also works on them, which is one of the crucial reasons there has been so much progress in AI in the last 10 years. \n",
                "\n",
                "Let's recall our simple neural network architecture that we studied prevously, a feedforward neural network of depth $3$. The equation defining the network is given by:\n",
                "\n",
                "$$\\mathbf{y} = \\mathbf{W}_3\\sigma(\\mathbf{W}_2\\sigma(\\mathbf{W}_1\\mathbf{x}+\\mathbf{b}_1)+\\mathbf{b}_2)+\\mathbf{b}_3$$\n",
                "\n",
                "where $\\mathbf{x}$ is the input vector, $\\mathbf{y}$ is the neural network prediction, $\\mathbf{W}_i$ and $\\mathbf{b}_i$ are the weight matrices and bias vectors of the network, and $\\sigma$ is the non-linear activation function. Recall that the weights and biases are the parameters that are updated when the network is trained to do something useful.\n",
                "\n",
                "So why is autodiff important here? This function looks complicated, but it's just a composition of affine transformations and elementwise non-linearities. We know the derivative of each of these functions, so we can use the chain rule to compute the derivative of the entire network, which is crucial for training. For functions that are compositions of many smaller functions, this is much more efficient than using finite differences and it has less error."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
